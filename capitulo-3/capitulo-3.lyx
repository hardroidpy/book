#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\use_default_options false
\maintain_unincluded_children false
\language spanish
\language_package none
\inputencoding utf8
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language french
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Aprendizaje Automático
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap3:Aprendizaje-Automatico"

\end_inset


\end_layout

\begin_layout Section
Introducción
\end_layout

\begin_layout Standard
En el contexto de aprendizaje automático, los patrones deben ser descubierto
 a partir de una serie de ejemplos que son denominados instancias.
 Tal conjunto de entrada se denomina conjunto de entrenamiento.
 En nuestro caso específico, cada instancia es un vector de características
 extraída de señales en una determinada ventana de tiempo.
 Los ejemplos en el conjunto de entrenamiento pueden o no pueden ser etiquetados
, es decir, asociada a una clase conocida (por ejemplo, caminar, correr,
 etc.).
 En algunos casos, tener datos etiquetados no es factible, ya que puede
 requerir un experto para examinar manualmente los ejemplos y asignar una
 etiqueta en base a su experiencia.
\end_layout

\begin_layout Standard
Este proceso es generalmente tedioso, caro y consume mucho tiempo en muchas
 aplicaciones de minería de datos.
 Existen dos enfoques de aprendizaje, es decir, aprendizaje supervisado
 y no supervisado, que se ocupan de datos etiquetados y no etiquetados,
 respectivamente.
 Puesto que un sistema de reconocimiento de la actividad humana debe devolver
 un marcador tal como caminar, sentarse, correr, etc., la mayoría de los
 sistemas de HAR utilizan modelos de aprendizaje supervisados.
 De hecho, podría ser muy difícil de discriminar actividades en un contexto
 completamente sin supervisión.
 Algunos otros sistemas funcionan de una manera semisupervisada en donde
 parte de los datos están sin etiqueta.
\end_layout

\begin_layout Section
Aprendizaje supervisado
\end_layout

\begin_layout Standard
El etiquetado de datos detectados a partir de individuos que realizan diferentes
 actividades es una tarea relativamente fácil.
 Algunos sistemas guardan datos del sensor en medios no volátiles mientras
 que una persona del equipo de investigación supervisa el proceso de recolección
 y de forma manual registra y etiqueta la actividad en cada intervalo de
 tiempo.
 Otros sistemas se caracterizan por una aplicación móvil que permite al
 usuario seleccionar la actividad que se realiza a partir de una lista,
 de esta manera, cada muestra se corresponde con una etiqueta de actividad
 y, a continuación, se almacena en el servidor.
 Este ultimo utilizado en este trabajo.
\end_layout

\begin_layout Standard
El aprendizaje supervisado ha sido un campo muy productivo, dando lugar
 a un gran número de algoritmos.
 La Tabla V resume los clasificadores más importantes en el reconocimiento
 de la actividad humana y su descripción se incluye a continuación.
\end_layout

\begin_layout Standard
**insertar listado de algoritmos**
\end_layout

\begin_layout Standard
En las siguientes secciones nos concentramos en el árbol de decisión, el
 cual fue utilizado para este trabajo.
\end_layout

\begin_layout Section
Arboles de Decisión
\end_layout

\begin_layout Standard
El árbol de decisión es un método de aprendizaje predictivo de una conjunto
 de tuplas o instancias etiquetadas.
 Un árbol de decisión es una estructura de árbol similar a un diagrama de
 flujo, donde cada nodo interno (nodo no hoja) denota una prueba de un atributo,
 y cada rama representa el camino de a seguir luego de la evaluación, y
 cada nodo hoja es una etiqueta o clase.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement !htbp
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
 
\begin_inset Graphics
	filename graphics/ad_1.png
	width 70line%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Árbol de decisión
\end_layout

\end_inset

Árbol de decisión
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig31:arbolEjemplo"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
En la figura por ejemplo un típico árbol de decisión que representa el concepto
 de jugar o no Golf, teniendo en cuenta variables climática representadas
 en los nodos internos, y los nodos hojas denotan las decisiones de jugar
 o no golf.
\end_layout

\begin_layout Section
Algoritmo C4.5
\end_layout

\begin_layout Standard
J.R Quinlan propone un mejora, una extensión al algoritmo ID3, al que denomina
 C4.5, este algoritmo genera un árbol de decisión a partir de los datos mediante
 particiones realizadas recursivamente.
 El árbol se construye mediante la estrategia profundidad-primero (depth-first).
\end_layout

\begin_layout Standard
El algoritmo C4.5 utiliza técnica heurística conocida como proporción de
 ganancia (gain-ratio).
 Es una medida basada en información que considera diferentes números y
 probabilidades de los resultados de las pruebas.
\end_layout

\begin_layout Standard
El algoritmo considera todas las pruebas posibles que puede dividir el conjunto
 de datos, seleccionar la prueba que le haya generado mayor ganancia de
 información.
 Para cada atributo discreto, se considera una prueba con N resultados,
 siendo N el numero de valores posibles que puede tomar el atributo.
 Para cada atributo continuo, se realiza la prueba binaria (1,0) sobre cada
 uno de los valores que puede tomar el atributo de los datos.
\end_layout

\begin_layout Section
Características del algoritmo C4.5
\end_layout

\begin_layout Itemize
Permite trabaja con valores continuos para los atributos, separando los
 posibles resultados en dos ramas 
\begin_inset Formula $A_{i}<=N$
\end_inset

 y 
\begin_inset Formula $A_{i}>N$
\end_inset

 .
 
\end_layout

\begin_layout Itemize
Los arboles son menos frondosos, ya que cada hoja cubre una distribución
 de clases no una clase en particular.
 
\end_layout

\begin_layout Itemize
Utiliza el método 'divide y vencerás' para generar el árbol de decisión
 incial a partir de un conjunto de datos de entrenamiento.
 
\end_layout

\begin_layout Itemize
Se basan en la utilización del criterio de proporción de ganancia, definido
 como 
\begin_inset Formula $I(X_{i},C)/I(X_{i})$
\end_inset

.
 De esta manera se consigue evitar que la variables con mayor numero de
 categorías salgan beneficiadas en la selección.
 
\end_layout

\begin_layout Itemize
Es recursivo.
 
\end_layout

\begin_layout Section
Información de Ganancia / Entropía 
\end_layout

\begin_layout Standard
C4.5 como su predecesor ID3 utiliza la entropía como medida de selección
 para el atributo.
 Dado un nodo 
\begin_inset Formula $N$
\end_inset

 que representa las tuplas de la partición 
\begin_inset Formula $D$
\end_inset

.
 El atributo con mayor valor de ganancia es elegido para la división de
 
\begin_inset Formula $N$
\end_inset

, el atributo reduce al mínimo la información necesaria para clasificar
 las tuplas de las particiones resultantes y refleja la menos aleatoriedad
 o impureza de la partición.
 Este enfoque minimiza el número esperado de los ensayos necesarios para
 clasificar una tupla dada y garantiza que un árbol simple (pero no necesariamen
te el más simple) se encuentre.
\end_layout

\begin_layout Standard
La información de ganancia necesaria para clasificar una tupla en 
\begin_inset Formula $D$
\end_inset

 es igual a:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Info(D)=-{\displaystyle \sum_{i=1}^{m}p_{i}\log_{2}(p_{i})}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
donde 
\begin_inset Formula $p_{i}$
\end_inset

 es la probabilidad de una tupla arbitraria en 
\begin_inset Formula $D$
\end_inset

 que pertenece a la clase 
\begin_inset Formula $C_{i}$
\end_inset

 se estima que es 
\begin_inset Formula $\lvert C_{i,D}unexpected''inmath\rvert/\lvert D\rvert$
\end_inset

.
 Se utiliza un logaritmo en base 2 porque la información esta codificada
 en bits.
 
\begin_inset Formula $Info(D)$
\end_inset

 es la cantidad media de información necesaria para identificar la etiqueta
 de una tupla dada en 
\begin_inset Formula $D$
\end_inset

.
 Nótese, que la información que tenemos se basa solamente en las proporciones
 de tuplas de cada clase.
 
\begin_inset Formula $Info(D)$
\end_inset

 también se conoce como entropía de 
\begin_inset Formula $D$
\end_inset


\end_layout

\begin_layout Standard
Ahora, supongamos que estábamos para particionar las tuplas en 
\begin_inset Formula $D$
\end_inset

 en algunos atribuyen A teniendo 
\begin_inset Formula $v$
\end_inset

 valores distintos 
\begin_inset Formula $\{a_{1},a_{2},...,a_{v}\}$
\end_inset

, como se observa a partir de los datos de entrenamiento.
 Si A tiene valores discretos, estos valores se corresponden directamente
 con los resultados de una prueba de 
\begin_inset Formula $v$
\end_inset

 sobre 
\begin_inset Formula $A$
\end_inset

.
 El atributo 
\begin_inset Formula $A$
\end_inset

 se puede utilizar para dividir 
\begin_inset Formula $D$
\end_inset

 en 
\begin_inset Formula $v$
\end_inset

 particiones o subconjuntos, 
\begin_inset Formula $\{D_{1},D_{2},...,D_{v}\}$
\end_inset

, donde 
\begin_inset Formula $D_{j}$
\end_inset

 contiene aquellas tuplas en 
\begin_inset Formula $D$
\end_inset

 que tienen 
\begin_inset Formula $a_{j}$
\end_inset

 resultados de 
\begin_inset Formula $A$
\end_inset

.
 Estas particiones se corresponderían con las ramas que crecen a partir
 del nodo 
\begin_inset Formula $N$
\end_inset

.
 Idealmente, nos gustaría que esta división para producir una clasificación
 exacta de las tuplas.
 Es decir, nos gustaría para cada partición sea pura.
 Sin embargo, es bastante probable que las particiones sea impura (por ejemplo,
 cuando una partición puede contener una colección de tuplas de diferentes
 clases en lugar de una sola clase).
 ¿Cuánto más información sería todavía necesaria (después de la partición)
 con el fin de llegar a una clasificación exacta? Esta cantidad se mide
 por:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Info_{A}(D)={\displaystyle \sum_{j=1}^{v}{\displaystyle \frac{\lvert D_{j}\rvert}{\lvert D\rvert}\times Info(D_{j})}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
El termino 
\begin_inset Formula ${\displaystyle \frac{\lvert D_{j}\rvert}{\lvert D\rvert}}$
\end_inset

 actúa como el peso de la j-esima partición.
 
\begin_inset Formula $Info_{A}(D)$
\end_inset

 es la información esperada requerida para clasificar la tupla de 
\begin_inset Formula $D$
\end_inset

 basada en la partición de 
\begin_inset Formula $A$
\end_inset

.
 Cuando menor sea la información esperada, mayor es la pureza de las particiones.
\end_layout

\begin_layout Standard
La información de ganancia se define como la diferencia entre lo requerido
 de información original (basado en la proporción de clases) y el nuevo
 requerimiento, obtenido luego de realizar la partición en 
\begin_inset Formula $A$
\end_inset

.
 Es decir,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Gain(A)=Info(D)-Info_{A}(D).
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
En otras palabras, 
\begin_inset Formula $Gain(A)$
\end_inset

 nos dice cuanto se ganaría por la ramificación en 
\begin_inset Formula $A$
\end_inset

.
 Es la reducción esperada en el requisito de información causado por conocer
 el valor de 
\begin_inset Formula $A$
\end_inset

.
 El atributo A con ganancia mas alta, 
\begin_inset Formula $Gain(A)$
\end_inset

, se selecciona como atributo de división en el nodo 
\begin_inset Formula $N$
\end_inset

.
 Esto equivale a decir que queremos particionar en el atributo A que haría
 la mejor clasificación, por lo que la cantidad de información aun necesaria
 para clasificar las tuplas sea mínima, 
\begin_inset Formula $InfoA(D)$
\end_inset

.
\end_layout

\begin_layout Section
Algoritmo
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Árbol de Decisión - C4.5
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "algoC45"

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

	
\backslash
begin{algorithmic}[1]
\end_layout

\begin_layout Plain Layout

		
\backslash
Require Conjunto de datos etiquetados $D$
\end_layout

\begin_layout Plain Layout

		
\backslash
Procedure{C4.5}{$ D $}
\end_layout

\begin_layout Plain Layout

			
\backslash
If {$D > 
\backslash
textit{es puro o cumple el criterio de parada} $} 
\end_layout

\begin_layout Plain Layout

				
\backslash
State
\backslash
textit{Termina}
\end_layout

\begin_layout Plain Layout

			
\backslash
EndIf
\end_layout

\begin_layout Plain Layout

			
\backslash
ForAll{$a 
\backslash
in D $}
\end_layout

\begin_layout Plain Layout

				
\backslash
State $
\backslash
textit{Computar información de division en a }$
\end_layout

\begin_layout Plain Layout

			
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

			
\backslash
State $ a_{best} =$ Mejor atributo de división respecto al criterio 
\end_layout

\begin_layout Plain Layout

			
\backslash
State $ Arbol =$ Crear un nodo de decisión con $ a_{best} $ en la raíz 
\end_layout

\begin_layout Plain Layout

			
\backslash
State $ D_{v} =$ Introducir sub-conjunto de $D$ basado en división $ a_{best}
 $
\end_layout

\begin_layout Plain Layout

			
\backslash
ForAll{$ D_{v} $}
\end_layout

\begin_layout Plain Layout

				
\backslash
State $ Arbol_{v} = C4.5(D_{v}) $
\end_layout

\begin_layout Plain Layout

				
\backslash
State Unir $ Arbol_{v} $ al correspondiente arco del Árbol
\end_layout

\begin_layout Plain Layout

			
\backslash
EndFor
\end_layout

\begin_layout Plain Layout

			
\backslash
State 
\end_layout

\begin_layout Plain Layout

			
\backslash
Return $ Arbol $
\end_layout

\begin_layout Plain Layout

		
\backslash
EndProcedure
\end_layout

\begin_layout Plain Layout

	
\backslash
end{algorithmic}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
