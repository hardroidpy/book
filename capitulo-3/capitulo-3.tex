
\chapter{Aprendizaje Automático}
\label{chap:Aprendizaje-Automatico}

\section{Introducción}
En el contexto de aprendizaje automático, los patrones deben ser descubierto a partir de una serie de ejemplos que son denominados instancias. Tal conjunto de entrada se denomina conjunto de entrenamiento. En nuestro caso específico, cada instancia es un vector de características extraída de señales en una determinada ventana de tiempo. Los ejemplos en el conjunto de entrenamiento pueden o no pueden ser etiquetados, es decir, asociada a una clase conocida (por ejemplo, caminar, correr, etc.). En algunos casos, tener datos etiquetados no es factible, ya que puede requerir un experto para examinar manualmente los ejemplos y asignar una etiqueta en base a su experiencia.

Este proceso es generalmente tedioso, caro y consume mucho tiempo en muchas aplicaciones de minería de datos. Existen dos enfoques de aprendizaje, es decir, aprendizaje supervisado y no supervisado, que se ocupan de datos etiquetados y no etiquetados, respectivamente. Puesto que un sistema de reconocimiento de la actividad humana debe devolver un marcador tal como caminar, sentarse, correr, etc., la mayoría de los sistemas de HAR utilizan modelos de aprendizaje supervisados. De hecho, podría ser muy difícil de discriminar actividades en un contexto completamente sin supervisión. Algunos otros sistemas funcionan de una manera semisupervisada en donde parte de los datos están sin etiqueta.

\section{Aprendizaje supervisado}
El etiquetado de datos detectados a partir de individuos que realizan diferentes actividades es una tarea relativamente fácil.
Algunos sistemas guardan datos del sensor en medios no volátiles mientras que una persona del equipo de investigación supervisa el proceso de recolección y de forma manual registra y etiqueta la actividad en cada intervalo de tiempo. Otros sistemas se caracterizan por una aplicación móvil que permite al usuario seleccionar la actividad que se realiza a partir de una lista, de esta manera, cada muestra se corresponde con una etiqueta de actividad y, a continuación, se almacena en el servidor. Este ultimo utilizado en este trabajo.

El aprendizaje supervisado ha sido un campo muy productivo, dando lugar a un gran número de algoritmos. La Tabla V resume los clasificadores más importantes en el reconocimiento de la actividad humana y su descripción se incluye a continuación.

**insertar listado de algoritmos**

En las siguientes secciones nos concentramos en el árbol de decisión, el cual fue utilizado para este trabajo.

\section{Arboles de Decisión}
El árbol de decisión es un método de aprendizaje predictivo de una conjunto de tuplas o instancias etiquetadas. Un árbol de decisión es una estructura de árbol similar a un diagrama de flujo, donde cada nodo interno (nodo no hoja) denota una prueba de un atributo, y cada rama representa el camino de a seguir luego de la evaluación, y cada nodo hoja es una etiqueta o clase.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{capitulo-3/graphics/ad_2}
	\caption[Árbol de decisión]{Árbol de decisión}
	\label{fig:arbolEjemplo}
\end{figure}

En la figura por ejemplo un típico árbol de decisión que representa el concepto de jugar o no Golf, teniendo en cuenta variables climática representadas en los nodos internos, y los nodos hojas denotan las decisiones de jugar o no golf. 

\section{Algoritmo C4.5}
J.R Quinlan propone un mejora, una extensión al algoritmo ID3, al que denomina C4.5, este algoritmo genera un árbol de decisión a partir de los datos mediante particiones realizadas recursivamente. El árbol se construye mediante la estrategia profundidad-primero (depth-first).

El algoritmo C4.5 utiliza técnica heurística conocida como proporción de ganancia (gain-ratio). Es una medida basada en información que considera diferentes números y probabilidades de los resultados de las pruebas. 

El algoritmo considera todas las pruebas posibles que puede dividir el conjunto de datos, seleccionar la prueba que le haya generado mayor ganancia de información. Para cada atributo discreto, se considera una prueba con N resultados, siendo N el numero de valores posibles que puede tomar el atributo. Para cada atributo continuo, se realiza la prueba binaria (1,0) sobre cada uno de los valores que puede tomar el atributo de los datos.

\section{Características del algoritmo C4.5}
\begin{itemize}
	\item Permite trabaja con valores continuos para los atributos, separando los posibles resultados en dos ramas $ A_{i} <= N $ y $ A_{i} > N $ . 
	\item Los arboles son menos frondosos, ya que cada hoja cubre una distribución de clases no una clase en particular.
	\item Utiliza el método 'divide y vencerás' para generar el árbol de decisión incial a partir de un conjunto de datos de entrenamiento.
	\item Se basan en la utilización del criterio de proporción de ganancia, definido como $ I(X_{i},C) / I(X_{i})  $. De esta manera se consigue evitar que la variables con mayor numero de categorías salgan beneficiadas en la selección. 
	\item Es recursivo.
\end{itemize}

\section{Información de Ganancia / Entropía }
C4.5 como su predecesor ID3 utiliza la entropía como medida de selección para el atributo. 
Dado un nodo $N$ que representa las tuplas de la partición $D$. El atributo con mayor valor de ganancia es elegido para la división de $N$, el atributo reduce al mínimo la información necesaria para clasificar las tuplas de las particiones resultantes y refleja la menos aleatoriedad o impureza de la partición. Este enfoque minimiza el número esperado de los ensayos necesarios para clasificar una tupla dada y garantiza que un árbol simple (pero no necesariamente el más simple) se encuentre.

La información de ganancia necesaria para clasificar una tupla en $D$ es igual a:

\begin{equation}
Info(D) = - \displaystyle\sum_{i=1}^{m} p_{i}\log_2(p_{i}) 
\end{equation}

donde $p_{i}$ es la probabilidad de una tupla arbitraria en $D$ que pertenece a la clase $C_{i}$ se estima que es $ \lvert C_{i,D}} \rvert / \lvert D \rvert $. Se utiliza un logaritmo en base 2 porque la información esta codificada en bits. $Info(D)$ es la cantidad media de información necesaria para identificar la etiqueta de una tupla dada en $D$. Nótese, que la información que tenemos se basa solamente en las proporciones de tuplas de cada clase. $Info(D)$ también se conoce como entropía de $D$

Ahora, supongamos que estábamos para particionar las tuplas en $D$ en algunos atribuyen A teniendo $v$ valores distintos $ \{ a_{1},a_{2},...,a_{v} \}$, como se observa a partir de los datos de entrenamiento. Si A tiene valores discretos, estos valores se corresponden directamente con los resultados de una prueba de $v$ sobre $A$. El atributo $A$ se puede utilizar para dividir $D$ en $v$ particiones o subconjuntos, $ \{ D_{1},D_{2},...,D_{v} \}$, donde $D_j$ contiene aquellas tuplas en $D$ que tienen $a_j$ resultados de $A$. Estas particiones se corresponderían con las ramas que crecen a partir del nodo $N$. Idealmente, nos gustaría que esta división para producir una clasificación exacta de las tuplas. Es decir, nos gustaría para cada partición sea pura. Sin embargo, es bastante probable que las particiones sea impura (por ejemplo, cuando una partición puede contener una colección de tuplas de diferentes clases en lugar de una sola clase). ¿Cuánto más información sería todavía necesaria (después de la partición) con el fin de llegar a una clasificación exacta? Esta cantidad se mide por:

\begin{equation}
Info_{A}(D) = \displaystyle\sum_{j=1}^{v} \displaystyle\frac{\lvert D_{j} \rvert}{\lvert D \rvert} \times Info(D_{j})
\end{equation}

El termino $\displaystyle\frac{\lvert D_{j} \rvert}{\lvert D \rvert}$ actúa como el peso de la j-esima partición. $Info_{A}(D)$ es la información esperada requerida para clasificar la tupla de $D$ basada en la partición de $A$. Cuando menor sea la información esperada, mayor es la pureza de las particiones.

La información de ganancia se define como la diferencia entre lo requerido de información original (basado en la proporción de clases) y el nuevo requerimiento, obtenido luego de realizar la partición en $A$. Es decir,

\begin{equation}
Gain(A) = Info(D) - Info_{A}(D).
\end{equation}

En otras palabras, $Gain(A)$ nos dice cuanto se ganaría por la ramificación en $A$. Es la reducción esperada en el requisito de información causado por conocer el valor de $A$. El atributo A con ganancia mas alta, $Gain(A)$, se selecciona como atributo de división en el nodo $N$. Esto equivale a decir que queremos particionar en el atributo A que haría la mejor clasificación, por lo que la cantidad de información aun necesaria para clasificar las tuplas sea minima, $InfoA(D)$.


\section{Algoritmo}

\begin{algorithm}
	\caption{Árbol de Decisión - C4.5}
	\label{algoC45}
	\begin{algorithmic}[1]
		\Require Conjunto de datos etiquetados $D$
		\Procedure{C4.5}{$ D $}
			\If {$D > \textit{es puro o cumple el criterio de parada} $} 
				\State\textit{Termina}
			\EndIf
			\ForAll{$a \in D $}
				\State $\textit{Computar información de division en a }$
			\EndFor
			\State $ a_{best} =$ Mejor atributo de división respecto al criterio 
			\State $ Arbol =$ Crear un nodo de decisión con $ a_{best} $ en la raíz 
			\State $ D_{v} =$ Introducir sub-conjunto de $D$ basado en división $ a_{best} $
			\ForAll{$ D_{v} $}
				\State $ Arbol_{v} = C4.5(D_{v}) $
				\State Unir $ Arbol_{v} $ al correspondiente arco del Árbol
			\EndFor
			\State 
			\Return $ Arbol $
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
